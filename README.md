# AMD_Robotics_Hackathon_2025_[TicTacToe]

## Team Information

**Team:** *Victor Gomez, Eric Huang, Ammar Moise, Leo Pouilly*

**Summary:** *SO101 playing to tic-tac-toe*

<p align="left">
  <img src="media/tic_tac_0.jpeg" alt="Morpion demo" width="45%"/>
</p>

*setup photo*

## Submission Details

### 1. Mission Description
- *board gaming*

### 2. Creativity
- *End-to-end LeRobot policy use. 

### 3. Technical implementations
- *Teleoperation / Dataset capture*
    - *<Image/video of teleoperation or dataset capture>*
- *Training*
    - Data integrity issues
- *Inference*
    NA

### 4. Ease of use
- *How generalizable is your implementation across tasks or environments?*
- *Flexibility and adaptability of the solution*
- *Types of commands or interfaces needed to control the robot*

## Additional Links
*For example, you can provide links to:*

- *Link to a video of your robot performing the task*
- *URL of your dataset in Hugging Face*
- *URL of your model in Hugging Face*
- *Link to a blog post describing your work*

## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.






















## TicTacToc Robot – AMD Robotics Hackathon 2025

This repo contains our code and configs to control a SO-101 robotic arm that plays Tic‑tac‑toe on a 3×3 board during the AMD Paris 2025 hackathon.

The idea is simple: you put tokens on a real board, the robot recognise the game, chooses a move, and places its own token.

<p align="left">
  <img src="media/tic_tac_0.jpeg" alt="Morpion demo" width="45%"/>
</p>

---

## Team

- Victor Gomez  
- Eric Huang  
- Ammar Moise  
- Leo Pouilly  

We tried to build a small end‑to‑end robotics project: perception → decision → motion, using a full LeRobot Policy.
---

## What this project aims

- **Play TicTacToe on a real 3×3 board**
  - Detect the board and tokens with a camera.
  - Decide the next move with simple game logic (no deep RL).
  - Control a SO‑101 arm to pick and place tokens.
- **Setup**
  - Human vs robot  

Under the hood we use imitation learning for low‑level manipulation (reach → grasp → place).

---

We rely on [LeRobot](https://github.com/huggingface/lerobot) for dataset handling, training and deployment.

---

## Requirements
Cf [LeRobot](https://github.com/huggingface/lerobot) requirements for SO100

Make sure your LeRobot environment is activated before running anything.


## Important files

- `scripts/play_morpion.py` – main entry point (simulation + hardware).  
- `scripts/vision.py` – board and token detection.  
- `mission/code/` – mission‑specific code (imitation learning, training, etc.).  
- `mission/wandb/` – last W&B run associated with the model used in the demo.  

---

## Vision & perception

The `scripts/vision.py` module is intentionally kept simple so you can adapt it easily:

- detect the 3×3 board,  
- detect tokens / played cells,  
- expose a clean 3×3 game state to the rest of the system.  

You can change the method (background subtraction, color detection, ArUco, small learned model, …) depending on your setup.

---

## Data 

For manipulation we use imitation learning:

- a few teleoperated demos (placing tokens on reference cells),  
- a network that outputs end‑effector displacement + gripper command,  
- training with MSE on trajectories using LeRobot + PyTorch,  
- experiment tracking with Weights & Biases.  


## Training 


## Current limitations

- Board calibration is still a bit manual.  
- We assume a fixed board, well framed in the camera field of view.  

Next steps:
- Run 
- more robust and automatic board detection,  
- more varied demonstrations and scenarios,  
- better collision handling and grasping of different tokens.  

---


